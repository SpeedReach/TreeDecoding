{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096e9876-50c2-4027-b3a1-c8b4fdbcee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from modeling_llama import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e86d74-e7d4-4f1e-b32a-0e760c2797bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Adjust based on your access\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b49d80-f432-4907-bcb6-70afe0ff4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "10f5bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class SearchNode:\n",
    "    def __init__(self, root, idx, token_id, token_score):\n",
    "        self.root: 'SearchTree' = root\n",
    "        self.idx: int = idx\n",
    "        self.token_id: Tensor = token_id\n",
    "        self.token_score: float = token_score\n",
    "        self.parent: Optional['SearchNode'] = None\n",
    "        self.children: List['SearchNode'] = []\n",
    "        self.acc_score: float = token_score\n",
    "\n",
    "\n",
    "    def add_children(self, child):\n",
    "        self.children.append(child)\n",
    "        child.parent = self\n",
    "        child.acc_score = self.acc_score + child.token_score\n",
    "        self.root.node_count += 1\n",
    "\n",
    "    def delete_child(self, child):\n",
    "        self.children.remove(child)\n",
    "        self.root.node_count -= 1\n",
    "\n",
    "\n",
    "class SearchTree:\n",
    "    def __init__(self, beam_width=3):\n",
    "        self.node_count: int = 0\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "        self.root: List[SearchNode] = []\n",
    "        self.beam_width: int = beam_width\n",
    "\n",
    "def generate_causal_mask(searchTree: SearchTree,input_len: int,nodes: List[SearchNode]) -> torch.Tensor:\n",
    "    branch_count = len(nodes)\n",
    "    mask = torch.full((1, 1, branch_count, searchTree.node_count + input_len), -65504, device=device, dtype=torch.float16)\n",
    "    mask[0, 0,:,:input_len] = 0\n",
    "    tmp = nodes.copy()\n",
    "    #print(\"========\")\n",
    "    while True:\n",
    "        end = False\n",
    "        for i in range(branch_count):\n",
    "            #print(i, tmp[i].idx)\n",
    "            mask[0, 0, i, tmp[i].idx + input_len] = 0\n",
    "            if tmp[i].parent is not None:\n",
    "                tmp[i] = tmp[i].parent\n",
    "            else:\n",
    "                end = True\n",
    "        if end:\n",
    "            return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "945ff9d5-cd6b-456d-a958-29ceae59879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "def prune_completed(searchTree: SearchTree, leaf: SearchNode):\n",
    "    prune_node = leaf\n",
    "    prune_nodes = []\n",
    "    while True:\n",
    "        prune_nodes.append(prune_node)\n",
    "        if prune_node.parent == None:\n",
    "            break\n",
    "        prune_node.parent.delete_child(prune_node)\n",
    "        if len(prune_node.parent.children) == 0:\n",
    "            break\n",
    "        prune_node = prune_node.parent\n",
    "    \n",
    "    \n",
    "def prune_kv_cache(past_key_values, prune_nodes: List[int]):\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_next_tokens(model, input_ids, beam_width = 3, max_tokens=300):\n",
    "    device = model.device\n",
    "    past_key_values = None\n",
    "    input_len = input_ids.shape[1]\n",
    "    print(\"input length: \", input_len)\n",
    "\n",
    "    #generate the first 3 tokens\n",
    "    outputs = model(input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "    past_key_values = outputs.past_key_values\n",
    "    token_scores = F.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    token_scores, tokens = torch.topk(token_scores, beam_width, dim=-1, largest=True, sorted=True)\n",
    "    searchTree = SearchTree(beam_width = beam_width)\n",
    "    newest_branch = []\n",
    "    idx = 0\n",
    "\n",
    "    #define eos token\n",
    "    eos_token_id = model.config.eos_token_id\n",
    "\n",
    "    \n",
    "    for i in range(beam_width):\n",
    "        searchNode = SearchNode(searchTree, idx, tokens[0][-1][i],token_scores[0][-1][i])\n",
    "        idx += 1\n",
    "        newest_branch.append(searchNode)\n",
    "        searchTree.root.append(searchNode)\n",
    "        searchTree.node_count += 1\n",
    "    \n",
    "    completed_branches = []\n",
    "    alive_beams = beam_width\n",
    "\n",
    "    for i in range(input_len, max_tokens):\n",
    "        #construct position_ids\n",
    "        print(\"alive_beams: \", alive_beams)\n",
    "\n",
    "        position_ids = torch.tensor([[i for _ in range(alive_beams)]], device=device)\n",
    "        \n",
    "        #construct attention_mask\n",
    "        attention_mask = generate_causal_mask(searchTree,input_len , newest_branch)\n",
    "        #print(attention_mask)\n",
    "        #print(attention_mask[0][0])\n",
    "\n",
    "        #construct input_ids\n",
    "        input_ids = torch.tensor([[node.token_id for node in newest_branch]], device=device)\n",
    "        \n",
    "        #generate candidate tokens\n",
    "        #print(\"kv: \", past_key_values[0][0].shape)\n",
    "        #print(\"atm: \", attention_mask.shape)\n",
    "        #print(\"node_count: \", searchTree.node_count)\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, position_ids=position_ids, attention_mask=attention_mask, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "        #calculate token scores\n",
    "        token_scores = F.log_softmax(outputs.logits, dim=-1)\n",
    "        token_scores, tokens = torch.topk(token_scores, alive_beams, dim=-1, largest=True, sorted=True)\n",
    "        #print(token_scores.shape)\n",
    "        #list n candidates from n branches\n",
    "        candidates = torch.empty(0, device=model.device)\n",
    "        candidate_scores = []\n",
    "        for i in range(alive_beams):\n",
    "            branch_score = newest_branch[i].acc_score\n",
    "            for j in range(alive_beams):\n",
    "                candidates = torch.cat((candidates, branch_score+token_scores[0][i][j].unsqueeze(0)))\n",
    "                candidate_scores.append(token_scores[0][i][j])\n",
    "        #print(candidates)\n",
    "        wa, token_idxs = torch.topk(candidates, alive_beams, dim=-1, largest=True, sorted=True)\n",
    "        #print(\"picks \", token_idxs, wa)\n",
    "\n",
    "        #update newest_branch and searchTree\n",
    "\n",
    "        tmp_newest_branch = []\n",
    "        \n",
    "        completed_nodes = []\n",
    "        for i in range(alive_beams):\n",
    "            token_idx = token_idxs[i]\n",
    "            token_id = tokens[0][int(token_idx/alive_beams)][int(token_idx%alive_beams)]\n",
    "            searchNode = SearchNode(searchTree, idx, token_id=token_id, token_score = candidate_scores[i])\n",
    "            \n",
    "            #print(int(token_idx/beam_width),\" add child\")\n",
    "            newest_branch[int(token_idx/alive_beams)].add_children(searchNode)\n",
    "            if token_id == eos_token_id:\n",
    "                print(searchNode.idx, \"ended\")\n",
    "                completed_nodes.append(searchNode)\n",
    "                completed_branches.append(searchNode)\n",
    "                searchTree.node_count -= 1\n",
    "                #tmp_newest_branch.append(searchNode)\n",
    "            else:\n",
    "                idx += 1\n",
    "                tmp_newest_branch.append(searchNode)\n",
    "                \n",
    "        alive_beams -= len(completed_nodes)\n",
    "        newest_branch = tmp_newest_branch\n",
    "    \n",
    "    #find the best branch\n",
    "    max_score=0\n",
    "    max_idx = 0\n",
    "    for i in range(alive_beams):\n",
    "        if newest_branch[i].acc_score > max_score:\n",
    "            max_score = newest_branch[i].acc_score\n",
    "            max_idx = i\n",
    "\n",
    "    #construct the output\n",
    "    outputs = []\n",
    "    newest_branch = newest_branch + completed_branches\n",
    "    for i in range(beam_width):\n",
    "        output = torch.empty(0, device=model.device)\n",
    "        branch_parent = newest_branch[i]\n",
    "        while branch_parent is not None:\n",
    "            output = torch.cat((output, branch_parent.token_id.unsqueeze(0)))\n",
    "            branch_parent = branch_parent.parent\n",
    "        output=output.flip(dims=[0])\n",
    "        outputs.append(output)\n",
    "        #outputs = torch.cat((outputs, output.unsqueeze(0)))\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83025a3b-e3da-4bd7-972c-1e724b4337da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length:  6\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "alive_beams:  4\n",
      "57 ended\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "alive_beams:  3\n",
      "139 ended\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      "alive_beams:  2\n",
      ": \n",
      "I'm so glad I got to spend it with you.\n",
      "I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm\n",
      ": \n",
      "I'm so glad I got to spend it with you.\n",
      "I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'm so glad I got to spend it with you. I'M\n",
      ": \n",
      "I'm so glad I got to spend it with you.</s>\n",
      ": \n",
      "I'm so glad I got to spend it with you.\n",
      "I'm so glad I got to spend it with you. I'm so glad I got to spend it with you.</s>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Such a nice day.\", return_tensors=\"pt\").to(model.device)\n",
    "beam_width = 4\n",
    "output = generate_next_tokens(model, input_ids, beam_width=beam_width, max_tokens=400)\n",
    "for i in range(beam_width):\n",
    "    print(\":\", tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82fcb190-505f-497b-b3ad-e90dda4ed42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_next_tokens(model, input_ids, beam_width = 5, max_tokens=100):\n",
    "    device = model.device\n",
    "    past_key_values = None\n",
    "    input_len = input_ids.shape[1]\n",
    "    print(\"input length: \", input_len)\n",
    "\n",
    "\n",
    "    outputs = model(input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "    past_key_values = outputs.past_key_values\n",
    "    token_scores = F.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    token_scores, tokens = torch.topk(token_scores, beam_width, dim=-1, largest=True, sorted=True)\n",
    "    searchTree = SearchTree(beam_width = beam_width)\n",
    "    newest_branch = []\n",
    "    round = 0\n",
    "    eos_token_id = model.config.eos_token_id\n",
    "    for i in range(beam_width):\n",
    "        searchNode = SearchNode(round, i,tokens[0][-1][i],token_scores[0][-1][i])\n",
    "        newest_branch.append(searchNode)\n",
    "        searchTree.root.append(searchNode)\n",
    "\n",
    "    completed_branches = []\n",
    "\n",
    "    for i in range(input_len, max_tokens):\n",
    "        #construct position_ids\n",
    "        position_ids = torch.tensor([[i for _ in range(beam_width)]], device=device)\n",
    "        \n",
    "        #construct attention_mask\n",
    "        attention_mask_length = input_len + (i-input_len+1) * beam_width\n",
    "        attention_mask = torch.full((1, 1, beam_width, attention_mask_length), -65504, device=device, dtype=torch.float16)\n",
    "        attention_mask[0,0,:,:input_len] = 0\n",
    "        for idx, node in enumerate(newest_branch):\n",
    "            node_parent = node\n",
    "            while node_parent is not None:\n",
    "                attention_mask[0, 0, idx, input_len + node_parent.round * beam_width + node_parent.sib_idx] = 0\n",
    "                node_parent = node_parent.parent\n",
    "\n",
    "        #print(\"mask\", attention_mask)\n",
    "        #print(\"pos_id\", position_ids)\n",
    "        #construct input_ids\n",
    "        input_ids = torch.tensor([[node.token_id for node in newest_branch]], device=device)\n",
    "        \n",
    "        #generate candidate tokens\n",
    "        outputs = model(input_ids, past_key_values=past_key_values, position_ids=position_ids, attention_mask=attention_mask, use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "        token_scores = F.log_softmax(outputs.logits, dim=-1)\n",
    "        token_scores, tokens = torch.topk(token_scores, beam_width, dim=-1, largest=True, sorted=True)\n",
    "        candidates = torch.empty(0, device=model.device)\n",
    "        for i in range(beam_width):\n",
    "            branch_score = newest_branch[i].acc_score\n",
    "            for j in range(beam_width):\n",
    "                candidates = torch.cat((candidates, branch_score+token_scores[0][i][j].unsqueeze(0)))\n",
    "        #print(candidates)\n",
    "        token_scores, token_idxs =torch.topk(candidates, beam_width, dim=-1, largest=True, sorted=True)\n",
    "        #print(\"selected\", token_idxs)\n",
    "\n",
    "        #update newest_branch and searchTree\n",
    "        round += 1\n",
    "        tmp_newest_branch = []\n",
    "        #print(\"new tokens\", tokens)\n",
    "        for i in range(beam_width):\n",
    "            token_idx = token_idxs[i]\n",
    "            token_id = tokens[0][int(token_idx/beam_width)][token_idx%beam_width]\n",
    "            searchNode = SearchNode(round=round,sib_idx=i, token_id=token_id, token_score = token_scores[i])\n",
    "            print(int(token_idx/beam_width),\" add child\")\n",
    "            newest_branch[int(token_idx/beam_width)].add_children(searchNode)\n",
    "            tmp_newest_branch.append(searchNode)\n",
    "        newest_branch = tmp_newest_branch\n",
    "    \n",
    "    #find the best branch\n",
    "    max_score=0\n",
    "    max_idx = 0\n",
    "    for i in range(beam_width):\n",
    "        if newest_branch[i].acc_score > max_score:\n",
    "            max_score = newest_branch[i].acc_score\n",
    "            max_idx = i\n",
    "\n",
    "    #construct the output\n",
    "    outputs = torch.empty(0, device=model.device)\n",
    "    for i in range(beam_width):\n",
    "        output = torch.empty(0, device=model.device)\n",
    "        branch_parent = newest_branch[i]\n",
    "        while branch_parent is not None:\n",
    "            output = torch.cat((output, branch_parent.token_id.unsqueeze(0)))\n",
    "            branch_parent = branch_parent.parent\n",
    "        output=output.flip(dims=[0])\n",
    "        outputs = torch.cat((outputs, output.unsqueeze(0)))\n",
    "    return outputs\n",
    "    \n",
    "    #construct the output\n",
    "    output = torch.empty(0, device=model.device)\n",
    "    best_branch_parent = newest_branch[max_idx]\n",
    "    while best_branch_parent is not None:\n",
    "        output = torch.cat((output, best_branch_parent.token_id.unsqueeze(0)))\n",
    "        best_branch_parent = best_branch_parent.parent\n",
    "    output=output.flip(dims=[0])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e82ac47-8805-4415-b77a-aecbedce7df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f27846-1ffc-4893-9056-7528333b11b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
